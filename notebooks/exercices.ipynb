{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/DEHMANIMOHAMMED/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf5facc-2a11-487d-c0c7-1862f0be9c25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exam_2025'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 59 (delta 21), reused 20 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (59/59), 1.41 MiB | 4.41 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 300"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimation des coefficients $$\\theta_k$$ avec la méthode des moindres carrés\n",
        "\n",
        "La méthode des moindres carrés est une approche simple et efficace pour estimer les coefficients $$\\theta_k$$ dans un modèle linéaire. Cette méthode consiste à minimiser la somme des carrés des résidus entre les valeurs prédites et les cibles. Le modèle linéaire est défini comme suit :\n",
        "\n",
        "$$\n",
        "t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon\n",
        "$$\n"
      ],
      "metadata": {
        "id": "xNwl89owWmgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from utils_exercices import generate_dataset\n",
        "\n",
        "# Charger les données\n",
        "train_set, test_set = generate_dataset(mySeed)\n",
        "\n",
        "# Préparer les données d'entraînement\n",
        "X_train = train_set['inputs']  # Prédicteurs\n",
        "t_train = train_set['targets']  # Cibles\n",
        "\n",
        "# Ajouter une colonne de 1 pour le biais\n",
        "X_train_aug = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
        "\n",
        "# Estimation des coefficients avec la formule des moindres carrés\n",
        "Theta = np.linalg.inv(X_train_aug.T @ X_train_aug) @ X_train_aug.T @ t_train\n",
        "\n",
        "# Afficher les coefficients\n",
        "print(\"Coefficients estimés (Theta):\", Theta)\n"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b20b7f6-19eb-4b07-c881-3581ff0f2ea0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients estimés (Theta): [15.01770145  2.97230274  2.9365536   5.85815285]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXGXg8tlPULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        # Couche d'entrée : 3 neurones (x, y, z)\n",
        "        # Couche de sortie : 1 neurone pour prédire t\n",
        "        # Pas de couches cachées car le modèle est linéaire\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Appliquer la transformation linéaire\n",
        "        return self.linear(x)\n"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        # Zero the gradient buffers\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = mySimpleNet(batch_inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(predictions, batch_targets.unsqueeze(1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print loss every 50 epochs\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfad9c36-0e70-49cc-fa9e-319e7005bb9a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/500], Loss: 226.9352\n",
            "Epoch [50/500], Loss: 3.9363\n",
            "Epoch [100/500], Loss: 3.0788\n",
            "Epoch [150/500], Loss: 4.4658\n",
            "Epoch [200/500], Loss: 3.7577\n",
            "Epoch [250/500], Loss: 4.2155\n",
            "Epoch [300/500], Loss: 3.9943\n",
            "Epoch [350/500], Loss: 4.3130\n",
            "Epoch [400/500], Loss: 4.4065\n",
            "Epoch [450/500], Loss: 3.6162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraire les poids et le biais\n",
        "theta_1_3 = mySimpleNet.linear.weight.data.squeeze().numpy()  # Poids pour x, y, z\n",
        "theta_0 = mySimpleNet.linear.bias.data.item()  # Biais (theta_0)\n",
        "\n",
        "# Afficher les coefficients estimés\n",
        "print(\"Estimations des coefficients (theta_k) :\")\n",
        "print(f\"theta_0 (biais) : {theta_0:.4f}\")\n",
        "print(f\"theta_1, theta_2, theta_3 (poids) : {theta_1_3}\")\n"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d13c0e-accc-4d2a-86ad-803d495782fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimations des coefficients (theta_k) :\n",
            "theta_0 (biais) : 15.0198\n",
            "theta_1, theta_2, theta_3 (poids) : [2.9730594 2.9340594 5.859893 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Données de test\n",
        "X_test = test_set['inputs']\n",
        "t_test = test_set['targets']\n",
        "\n",
        "# Prédictions de la méthode des moindres carrés (Q1)\n",
        "Theta_q1 = [15.01770145, 2.97230274, 2.9365536, 5.85815285]\n",
        "predictions_q1 = Theta_q1[0] + X_test @ Theta_q1[1:]\n",
        "\n",
        "# Prédictions du réseau neuronal (Q4)\n",
        "predictions_q4 = mySimpleNet(torch.tensor(X_test, dtype=torch.float32)).detach().numpy().squeeze()\n",
        "\n",
        "# Calcul des MSE\n",
        "mse_q1 = mean_squared_error(t_test, predictions_q1)\n",
        "mse_q4 = mean_squared_error(t_test, predictions_q4)\n",
        "\n",
        "# Résultats\n",
        "print(f\"Erreur quadratique moyenne (MSE) - Moindres carrés (Q1) : {mse_q1:.4f}\")\n",
        "print(f\"Erreur quadratique moyenne (MSE) - Réseau neuronal (Q4) : {mse_q4:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr6824bgZIHp",
        "outputId": "4d8cb440-72e3-4d45-93b4-02b8977d9061"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur quadratique moyenne (MSE) - Moindres carrés (Q1) : 4.0957\n",
            "Erreur quadratique moyenne (MSE) - Réseau neuronal (Q4) : 4.0967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparaison des performances entre les méthodes des moindres carrés (Q1) et le réseau neuronal (Q4)\n",
        "\n",
        "## Résultats\n",
        "- **Erreur quadratique moyenne (MSE) - Moindres carrés (Q1)** : 4.0957\n",
        "- **Erreur quadratique moyenne (MSE) - Réseau neuronal (Q4)** : 4.0967\n",
        "\n",
        "## Analyse\n",
        "1. **Performances similaires :**\n",
        "   - Les deux approches montrent des erreurs quadratiques moyennes très proches (moins de 0.001 de différence).\n",
        "   - Cela indique que les deux méthodes capturent efficacement la relation entre les prédicteurs et les cibles.\n",
        "\n",
        "2. **Léger avantage pour la méthode des moindres carrés :**\n",
        "   - La méthode analytique donne une MSE légèrement inférieure, car elle calcule directement les coefficients optimaux en minimisant explicitement l'erreur quadratique.\n",
        "\n",
        "3. **Impact du réseau neuronal :**\n",
        "   - Le réseau neuronal, entraîné par SGD, approche bien la solution optimale mais peut légèrement diverger en raison de :\n",
        "     - La stochasticité dans l'optimisation (SGD).\n",
        "     - La convergence approximative par itérations.\n",
        "     - Une sensibilité au taux d'apprentissage ou au nombre d'époques.\n",
        "\n",
        "## Conclusion\n",
        "- Pour ce problème simple de régression linéaire, la méthode des moindres carrés est légèrement plus précise, rapide et directe.\n",
        "- Cependant, le réseau neuronal reste une solution flexible qui pourrait être adapté à des problèmes non linéaires ou plus complexes.\n",
        "\n",
        "**Recommandation** : Utiliser les moindres carrés pour des problèmes simples et linéaires, et recourir à des réseaux neuronaux pour des cas plus complexes ou non linéaires.\n"
      ],
      "metadata": {
        "id": "xDh_YVsVZZxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbfd0f4-c869-4913-e57b-0ac82d6ea7da"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nb de paramètres dans self.Down1: (calcul \"à la main\")\n",
        "# Première convolution : 64 * 128 * 3 + 128\n",
        "# Première BatchNorm : 128 * 2\n",
        "# Deuxième convolution : 64 * 128 * 3 + 128\n",
        "# Deuxième BatchNorm : 128 * 2\n",
        "# Total : 49,920 paramètres\n",
        "\n",
        "# Nb de paramètres au total:\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "model = causalFCN()\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Nombre total de paramètres : {total_params}\")"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eba8ed8-d502-411d-a201-3dae87684477"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de paramètres : 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2 : Réduction et restitution de la taille du vecteur d'entrée**\n",
        "\n",
        "## Réduction de la taille (Encoder)\n",
        "La taille du vecteur d'entrée est réduite par les mécanismes suivants dans la première partie du réseau (encoder) :\n",
        "1. **Pooling causal (Down_causal)** :\n",
        "   - Réduit la taille temporelle grâce à des couches de pooling (e.g., `MaxPool1d`).\n",
        "   - Assure la causalité en ajustant le padding avant le pooling.\n",
        "\n",
        "2. **Dilation** :\n",
        "   - Les convolutions causales dilatées augmentent le champ réceptif sans réduire directement la taille, mais elles préparent l’information pour des résolutions plus basses.\n",
        "\n",
        "## Restitution de la taille (Decoder)\n",
        "La taille du vecteur d'entrée est restituée dans la deuxième partie du réseau (decoder) grâce aux mécanismes suivants :\n",
        "1. **Upsampling (Up_causal)** :\n",
        "   - Les couches `Up_causal` effectuent une interpolation ou une dé-convolution pour augmenter la résolution temporelle à chaque étape.\n",
        "\n",
        "2. **Concaténation des features** :\n",
        "   - À chaque étape, les features remontées sont concaténées avec celles du niveau correspondant dans l'encoder via des connexions skip, permettant une meilleure restitution des détails temporels.\n",
        "\n",
        "## Conclusion\n",
        "La réduction et la restitution de la taille sont soigneusement orchestrées pour préserver la causalité tout en capturant les dépendances à différentes échelles temporelles.\n"
      ],
      "metadata": {
        "id": "VXObenqOak-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Initialisation du modèle causal\n",
        "model = causalFCN()\n",
        "\n",
        "# Taille de l'entrée\n",
        "input_size = 10000\n",
        "\n",
        "# Création de deux entrées identiques\n",
        "input1 = torch.zeros(1, 1, input_size)  # Série temporelle nulle\n",
        "input2 = input1.clone()\n",
        "\n",
        "# Modifier uniquement la composante x[4500] de input2\n",
        "input2[0, 0, 4500] = 1.0\n",
        "\n",
        "# Passer les deux entrées dans le modèle\n",
        "output1 = model(input1)\n",
        "output2 = model(input2)\n",
        "\n",
        "# Calcul de la différence entre les sorties\n",
        "diff = (output1 - output2).abs()\n",
        "\n",
        "# Identifier la composante non nulle associée à y[5000]\n",
        "receptive_field_indices = torch.nonzero(diff[0, 0, 5000:], as_tuple=True)[0]\n",
        "receptive_field_start = 5000 - torch.nonzero(diff[0, 0, :5000], as_tuple=True)[0].max()\n",
        "\n",
        "# Champ réceptif\n",
        "receptive_field = receptive_field_indices.max().item() + receptive_field_start + 1\n",
        "\n",
        "print(f\"Taille du champ réceptif associé à y[5000] : {receptive_field}\")\n"
      ],
      "metadata": {
        "id": "69WMWCSZAg5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b0f9d4-c679-4c58-dca2-a84f2229ee3b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du champ réceptif associé à y[5000] : 5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeooRYE-ATGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG?token=GHSAT0AAAAAAC427DACOPGNDNN6UDOLVLLAZ4BB2JQ\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}